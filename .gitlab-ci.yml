image:
  name: docker/compose:1.24.1
  entrypoint: ["/bin/sh","-l","-c"]

services:
  - docker:dind

stages:
  # - test
  - deploy

# pytest:
#   stage: test
#   script:
#     - cd rest/tests/
#     - docker-compose up --abort-on-container-exit --exit-code-from pytest --build

deploy-production-rest-lambda:
  stage: deploy
  when: manual
  only:
    variables:
      - $CI_COMMIT_TAG =~ /^v[0-9]+[.][0-9]+[.][0-9]+$/
  image: python:3.6.6
  before_script:
    # configure aws access credentials:
    - mkdir -p ~/.aws
    - echo -e "[default]\nregion=eu-central-1" > ~/.aws/config
    - echo -e "[default]\naws_access_key_id=$PRODZAPPA_AWS_ACCESS_KEY_ID\naws_secret_access_key=$PRODZAPPA_AWS_SECRET_ACCESS_KEY" > ~/.aws/credentials
    # `pipenv run` doesn't currently set $VIRTUAL_ENV, and zappa is failing because of it. The workaround is to use .venv/ in
    # a project itself, and set it explicitly: https://github.com/Miserlou/Zappa/issues/1443#issuecomment-374174824
    - export PIPENV_VENV_IN_PROJECT=true
    - pip install --upgrade pipenv
    - export VIRTUAL_ENV=.venv/
  script:
    - cd rest/
    - PIPENV_IGNORE_VIRTUALENVS=1 pipenv install --dev
    # create zappa_settings.json on-the-fly:
    - cp zappa_settings.json.template zappa_settings.json
    - sed -i "s/@@DYNAMODB_PRODUCTION@@/yes/g" zappa_settings.json
    - sed -i "s/@@DYNAMODB_LOCAL_URL@@//g" zappa_settings.json  # not used
    - sed -i "s/@@AWS_ACCESS_KEY_ID@@/$PRODDATA_AWS_ACCESS_KEY_ID/g" zappa_settings.json
    - sed -i "s#@@AWS_SECRET_ACCESS_KEY@@#$PRODDATA_AWS_SECRET_ACCESS_KEY#g" zappa_settings.json
    - sed -i "s/@@S3_DATA_BUCKET@@/$PRODDATA_S3_BUCKET/g" zappa_settings.json
    - pipenv run zappa deploy production || pipenv run zappa update production
    # ensure tables are created:
    - export AWS_ACCESS_KEY_ID="$PRODDATA_AWS_ACCESS_KEY_ID"
    - export AWS_SECRET_ACCESS_KEY="$PRODDATA_AWS_SECRET_ACCESS_KEY"
    - pipenv run bash -c "DYNAMODB_PRODUCTION=yes python dynamodb.py"

deploy-production-workers-ecr:
  stage: deploy
  # when: manual
  # only:
  #   variables:
  #     - $CI_COMMIT_TAG =~ /^v[0-9]+[.][0-9]+[.][0-9]+$/
  variables:
    CI_COMMIT_TAG: v0.0.6
    CI_REGISTRY_IMAGE: shopeneo/workers
  before_script:
    # configure aws access credentials:
    - mkdir -p ~/.aws
    - echo -e "[default]\nregion=eu-central-1" > ~/.aws/config
    - echo -e "[default]\naws_access_key_id=$PRODWORKERS_AWS_ACCESS_KEY_ID\naws_secret_access_key=$PRODWORKERS_AWS_SECRET_ACCESS_KEY" > ~/.aws/credentials
    - apk add --update python python-dev py-pip jq
    - pip install awscli
  script:
    - cd workers/
    # build and publish docker image:
    - $(aws ecr get-login --no-include-email --region eu-central-1)
    - docker build -t "$CI_REGISTRY/$CI_REGISTRY_IMAGE:$CI_COMMIT_TAG" -t "$CI_REGISTRY/$CI_REGISTRY_IMAGE:latest" --build-arg VERSION=$CI_COMMIT_TAG --build-arg VCS_REF=$CI_COMMIT_SHA --build-arg BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ') .
    - docker push "$CI_REGISTRY/$CI_REGISTRY_IMAGE:$CI_COMMIT_TAG"
    - docker push "$CI_REGISTRY/$CI_REGISTRY_IMAGE:latest"
    # populate task definition file with env vars:
    - sed -i "s/@@DOCKER_IMAGE_VERSION@@/$CI_COMMIT_TAG/g" awsecs-task-definition.json
    - sed -i "s/@@DATA_AWS_ACCESS_KEY_ID@@/$PRODDATA_AWS_ACCESS_KEY_ID/g" awsecs-task-definition.json
    - sed -i "s#@@DATA_AWS_SECRET_ACCESS_KEY@@#$PRODDATA_AWS_SECRET_ACCESS_KEY#g" awsecs-task-definition.json
    - sed -i "s/@@SENTINELHUB_LAYER_ID@@/$SENTINELHUB_LAYER_ID/g" awsecs-task-definition.json
    - sed -i "s/@@SENTINELHUB_INSTANCE_ID@@/$SENTINELHUB_INSTANCE_ID/g" awsecs-task-definition.json
    - TASKDEF=$(aws ecs register-task-definition --region "eu-central-1" --cli-input-json file://$(pwd)/awsecs-task-definition.json)
    - echo $TASKDEF
    - TASK_REVISION=$(echo $TASKDEF | jq ".taskDefinition.revision")
    - #aws ecs list-tasks --cluster shopeneoworkers --desired-status RUNNING | jq ".taskArns[0]"
